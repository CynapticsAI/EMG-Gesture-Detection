{"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN5rIA9iLjdescapV+r7YE0","name":"","provenance":[{"file_id":"1IFCtFud6V4ZO67tW0biJFwxn22QH2luf","timestamp":1706173869013}],"version":""},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2355807,"sourceType":"datasetVersion","datasetId":1422521}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wget","metadata":{"executionInfo":{"elapsed":14305,"status":"ok","timestamp":1706293790593,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"3LZnnW5tnpRO","outputId":"e67ebf08-4388-42ec-998b-7c7267ca18f3","execution":{"iopub.status.busy":"2024-02-04T12:43:25.047008Z","iopub.execute_input":"2024-02-04T12:43:25.047462Z","iopub.status.idle":"2024-02-04T12:43:44.196960Z","shell.execute_reply.started":"2024-02-04T12:43:25.047420Z","shell.execute_reply":"2024-02-04T12:43:44.195532Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting wget\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: wget\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=5882ceabd98073f2c4775c13448e88913d64b1f3f622f053caf10066ef68ed96\n  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\nSuccessfully built wget\nInstalling collected packages: wget\nSuccessfully installed wget-3.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/Pradeep-Kumar-Rebbavarapu/EMG_data","metadata":{"executionInfo":{"elapsed":402,"status":"ok","timestamp":1706293790976,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"-NBpYbr_B3yG","outputId":"ade2328b-cf52-48ed-a8a9-bd6ce724b518","execution":{"iopub.status.busy":"2024-02-04T12:43:44.199772Z","iopub.execute_input":"2024-02-04T12:43:44.200162Z","iopub.status.idle":"2024-02-04T12:43:46.274673Z","shell.execute_reply.started":"2024-02-04T12:43:44.200127Z","shell.execute_reply":"2024-02-04T12:43:46.273504Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'EMG_data'...\nremote: Enumerating objects: 12, done.\u001b[K\nremote: Counting objects: 100% (12/12), done.\u001b[K\nremote: Compressing objects: 100% (12/12), done.\u001b[K\nremote: Total 12 (delta 4), reused 3 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (12/12), 5.58 KiB | 1.39 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tensorflow_addons","metadata":{"executionInfo":{"elapsed":5586,"status":"ok","timestamp":1706293796545,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"MEUjMc7euVZR","outputId":"11bcc2cc-a09b-4463-dada-3f9b9d5c227d","execution":{"iopub.status.busy":"2024-02-04T12:43:46.276730Z","iopub.execute_input":"2024-02-04T12:43:46.277104Z","iopub.status.idle":"2024-02-04T12:44:00.632854Z","shell.execute_reply.started":"2024-02-04T12:43:46.277056Z","shell.execute_reply":"2024-02-04T12:44:00.631530Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow_addons in /opt/conda/lib/python3.10/site-packages (0.23.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow_addons) (21.3)\nRequirement already satisfied: typeguard<3.0.0,>=2.7 in /opt/conda/lib/python3.10/site-packages (from tensorflow_addons) (2.13.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow_addons) (3.0.9)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install keras-lookahead","metadata":{"executionInfo":{"elapsed":4500,"status":"ok","timestamp":1706293801021,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"WrYcnky0sWj0","outputId":"08808085-dae6-484d-c085-86f83cbe10b6","execution":{"iopub.status.busy":"2024-02-04T12:44:00.636193Z","iopub.execute_input":"2024-02-04T12:44:00.636624Z","iopub.status.idle":"2024-02-04T12:44:17.802563Z","shell.execute_reply.started":"2024-02-04T12:44:00.636580Z","shell.execute_reply":"2024-02-04T12:44:17.801344Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting keras-lookahead\n  Downloading keras-lookahead-0.9.0.tar.gz (5.3 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras-lookahead) (1.24.3)\nRequirement already satisfied: Keras in /opt/conda/lib/python3.10/site-packages (from keras-lookahead) (2.13.1)\nBuilding wheels for collected packages: keras-lookahead\n  Building wheel for keras-lookahead (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-lookahead: filename=keras_lookahead-0.9.0-py3-none-any.whl size=6386 sha256=324f7ef5146c3e8daf16396d877501ce809e7d415b0182bd584bdac128b5a338\n  Stored in directory: /root/.cache/pip/wheels/f0/87/b1/07761aa9c1570e0cf31245fa80739d2c606b3f930d269dcade\nSuccessfully built keras-lookahead\nInstalling collected packages: keras-lookahead\nSuccessfully installed keras-lookahead-0.9.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport wget","metadata":{"executionInfo":{"elapsed":655,"status":"ok","timestamp":1706293801659,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"hMyrFKfpnacN","execution":{"iopub.status.busy":"2024-02-04T12:44:17.804260Z","iopub.execute_input":"2024-02-04T12:44:17.807128Z","iopub.status.idle":"2024-02-04T12:44:17.818587Z","shell.execute_reply.started":"2024-02-04T12:44:17.807078Z","shell.execute_reply":"2024-02-04T12:44:17.817442Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"bc_dict = {\n    2: {\n        0:0,\n        1:1,\n        2:2,\n        3:3,\n        4:4,\n        5:5,\n        6:6,\n        7:7,\n        8:8,\n        9:9,\n       10:10,\n       11:11,\n       12:12,\n       13:13,\n       14:14,\n       15:15,\n       16:16,\n       17:17,\n    },\n    3: {\n        0:0,\n        1:18,\n        2:19,\n        3:20,\n        4:21,\n        5:22,\n        6:23,\n        7:24,\n        8:25,\n        9:26,\n       10:27,\n       11:28,\n       12:29,\n       13:30,\n       14:31,\n       15:32,\n       16:33,\n       17:34,\n       18:35,\n       19:36,\n       20:37,\n       21:38,\n       22:39,\n       23:40,\n    }\n}\n","metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1706293801659,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"lEWVNzNRGfbO","execution":{"iopub.status.busy":"2024-02-04T12:44:17.819794Z","iopub.execute_input":"2024-02-04T12:44:17.820122Z","iopub.status.idle":"2024-02-04T12:44:17.829784Z","shell.execute_reply.started":"2024-02-04T12:44:17.820094Z","shell.execute_reply":"2024-02-04T12:44:17.828475Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"label_dict = {\n    1: {\n        0:0,\n        1:1,\n        2:2,\n        3:3,\n        4:4,\n        5:5,\n        6:6,\n        7:7,\n        8:8,\n        9:9,\n        10:10,\n        11:11,\n        12:12\n    },\n    2: {\n        0:0,\n        1:13,\n        2:14,\n        3:15,\n        4:16,\n        5:17,\n        6:18,\n        7:19,\n        8:20,\n        9:21,\n        10:22,\n        11:23,\n        12:24,\n        13:25,\n        14:26,\n        15:27,\n        16:28,\n        17:29,\n    },\n    3: {\n        0:0,\n        1:30,\n        2:31,\n        3:32,\n        4:33,\n        5:34,\n        6:35,\n        7:36,\n        8:37,\n        9:38,\n        10:39,\n        11:40,\n        12:41,\n        13:42,\n        14:43,\n        15:44,\n        16:45,\n        17:46,\n        18:47,\n        19:48,\n        20:49,\n        21:50,\n        22:51,\n        23:52,\n    }\n}\n","metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1706293801660,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"BwSJw1FOlyV2","execution":{"iopub.status.busy":"2024-02-04T12:44:17.831557Z","iopub.execute_input":"2024-02-04T12:44:17.832062Z","iopub.status.idle":"2024-02-04T12:44:17.844589Z","shell.execute_reply.started":"2024-02-04T12:44:17.832017Z","shell.execute_reply":"2024-02-04T12:44:17.843444Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import random\nimport multiprocessing\nimport numpy as np\nimport pandas as pd\nfrom scipy import signal\nfrom scipy.io import loadmat\n\n\n\ndef roll_labels(x, y):\n    labs_rolled = []\n    for i in range(len(y)):\n        l = y[i]\n        n = x[i].shape[0]\n        labs_rolled.append(np.repeat(l, n))\n    return np.hstack(labs_rolled)\n\n\ndef window_roll(a, stepsize=5, width=52):\n    n = a.shape[0]\n    emg = np.dstack([a[i : 1 + n + i - width : stepsize] for i in range(0, width)])\n    return emg\n\n\ndef add_noise_snr(signal, snr=25):\n    sgn_db = np.log10((signal ** 2).mean(axis=0)) * 10\n    noise_avg_db = sgn_db - snr\n    noise_variance = 10 ** (noise_avg_db / 10)\n    noise = np.random.normal(0, np.sqrt(noise_variance), signal.shape)\n    return signal + noise\n\nrlist = sum([[(x / 2) % 30] * ((x // 2) % 30) for x in range(120)], [])\n\n\ndef add_noise_random(signal):\n    num = random.choice(rlist)\n    return add_noise_snr(signal, num)\n\n\n\ndef moving_average(data_set, periods=3):\n    weights = np.ones(periods) / periods\n    return np.convolve(data_set, weights, mode=\"valid\")\n\n\ndef ma(window, n):\n    return np.vstack(\n        [moving_average(window[:, i], n) for i in range(window.shape[-1])]\n    ).T\n\n\ndef ma_batch(batch, n):\n    return np.dstack([ma(batch[i, :, :], n) for i in range(batch.shape[0])])\n\n\n\ndef _butter_highpass(cutoff, fs, order=3):\n    nyq = 0.5 * fs\n    normal_cutoff = cutoff / nyq\n    b, a = signal.butter(order, normal_cutoff, btype=\"high\", analog=False)\n    return b, a\n\n\ndef butter_highpass_filter(data, cutoff=2, fs=200, order=3):\n    b, a = _butter_highpass(cutoff=cutoff, fs=fs, order=order)\n    y = signal.lfilter(b, a, data)\n    return y\n\n\n\ndef first0(x):\n    return np.unique(x)[0]\n\n\ndef first_appearance(arr):\n\n    inn = [arr[i] for i in range(arr.shape[0])]\n    with multiprocessing.Pool(None) as p:\n        res = p.map(first0, inn)\n    return np.asarray(res)\n\n\nclass dataset(object):\n    def __init__(\n        self,\n        path,\n        butter=True,\n        rectify=True,\n        ma=15,\n        step=5,\n        window=52,\n        exercises=[\"a\", \"b\", \"c\"],\n        features=None,\n    ):\n        self.path = path\n        self.butter = butter\n        self.rectify = rectify\n        self.ma = ma\n        self.step = step\n        self.window = window\n        self.exercises = exercises\n        self.features = features\n\n\n        self.read_data()\n        self.process_data()\n\n    def _load_file(self, path, ex, features=None):\n        res = loadmat(path)\n        data = []\n        imu = res[\"acc\"].copy()\n        rep = res[\"rerepetition\"].copy()\n        emg = res[\"emg\"].copy()\n        lab = res[\"restimulus\"].copy()\n        if 'a' not in self.exercises:\n            lab = np.array([[bc_dict[ex][lab[i][0]]] for i in range(lab.shape[0])])\n        else:\n            lab = np.array([[label_dict[ex][lab[i][0]]] for i in range(lab.shape[0])])\n\n        del res\n\n        data.append(emg)\n        if features:\n            for ft in features:\n                print(\"adding features\")\n                sameDim = data[0].shape[0] == np.shape(res[ft])[0]\n                newData = []\n                if not sameDim and np.shape(res[ft])[1] == 1:\n                    newData = np.full((np.shape(data[0])[0], 1), res[ft][0, 0])\n                else:\n                    newData = res[ft]\n                data.append(newData)\n\n        return np.concatenate(data, axis=1), lab, rep, imu\n\n    def _load_by_trial(self, trial=1, features=None):\n        data = []\n        labs = []\n        reps = []\n        imu = []\n        for i in range(1, 11):\n            path = f\"{self.path}/s{i}/S{i}_E{trial}_A1.mat\"\n            emg, l, r, ii = self._load_file(path, ex=trial, features=features)\n            data.append(emg)\n            labs.append(l)\n            reps.append(r)\n            imu.append(ii)\n        return data, labs, reps, imu\n\n    def read_data(self):\n        ex_dict = dict(zip([\"a\", \"b\", \"c\"], range(1, 4)))\n        self.emg = []\n        self.labels = []\n        self.repetition = []\n        self.imu = []\n\n        for e in self.exercises:\n            exercise = ex_dict[e]\n            emg, lab, rep, imu = self._load_by_trial(trial=exercise, features=self.features)\n            self.emg += emg\n            self.labels += lab\n            self.repetition += rep\n            self.imu += imu\n        print(sum([x.shape[0] for x in self.emg]))\n\n    def process_data(self):\n        if self.rectify:\n            self.emg = [np.abs(x) for x in self.emg]\n\n        if self.butter:\n            self.emg = [butter_highpass_filter(x) for x in self.emg]\n\n        self.flat = [self.emg, self.labels, self.repetition, self.imu]\n        self.emg = [window_roll(x, self.step, self.window) for x in self.emg]\n        self.imu = [window_roll(x, self.step, self.window) for x in self.imu]\n        self.labels = [window_roll(x, self.step, self.window) for x in self.labels]\n        self.repetition = [window_roll(x, self.step, self.window) for x in self.repetition]\n\n        self.emg = np.moveaxis(np.concatenate(self.emg, axis=0), 2, 1)\n        self.imu = np.moveaxis(np.concatenate(self.imu, axis=0), 2, 1)\n        self.labels = np.moveaxis(np.concatenate(self.labels, axis=0), 2, 1)[..., -1]\n        self.repetition = np.moveaxis(np.concatenate(self.repetition, axis=0), 2, 1)[..., -1]\n\n        no_leaks = np.array(\n            [\n                i\n                for i in range(self.repetition.shape[0])\n                if np.unique(self.repetition[i]).shape[0] == 1\n            ]\n        )\n\n\n\n        self.emg = self.emg[no_leaks, :, :]\n        self.imu = self.imu[no_leaks, :, :]\n        self.labels = self.labels[no_leaks, :]\n        self.repetition = self.repetition[no_leaks, :]\n        self.labels = first_appearance(self.labels)\n        self.repetition = first_appearance(self.repetition)\n        self.emg = self.emg.astype(np.float16)\n        self.imu = self.imu.astype(np.float16)","metadata":{"executionInfo":{"elapsed":445,"status":"ok","timestamp":1706293802081,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"Vdf0fEQtNdM8","execution":{"iopub.status.busy":"2024-02-04T12:44:17.846642Z","iopub.execute_input":"2024-02-04T12:44:17.847046Z","iopub.status.idle":"2024-02-04T12:44:18.913334Z","shell.execute_reply.started":"2024-02-04T12:44:17.846998Z","shell.execute_reply":"2024-02-04T12:44:18.911751Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"if \"ninaPro\" not in os.listdir():\n    wget.download('https://www.dropbox.com/s/kxrqhqhcz367v77/nina.tar.gz?dl=1', out='.')\n    !tar -xzvf nina.tar.gz\n\ndata = dataset(\"./ninaPro\")","metadata":{"executionInfo":{"elapsed":49553,"status":"ok","timestamp":1706293851630,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"X7_aitRuGiHF","outputId":"a0275db8-4f33-4a1c-b25f-df3e75042785","execution":{"iopub.status.busy":"2024-02-04T12:44:18.914782Z","iopub.execute_input":"2024-02-04T12:44:18.915285Z","iopub.status.idle":"2024-02-04T12:46:59.590212Z","shell.execute_reply.started":"2024-02-04T12:44:18.915252Z","shell.execute_reply":"2024-02-04T12:46:59.588998Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"ninaPro/\nninaPro/s2/\nninaPro/s2/S2_E1_A1.mat\nninaPro/s2/S2_E3_A1.mat\nninaPro/s2/S2_E2_A1.mat\nninaPro/s3.zip\nninaPro/s10.zip\nninaPro/s6/\nninaPro/s6/S6_E1_A1.mat\nninaPro/s6/S6_E2_A1.mat\nninaPro/s6/S6_E3_A1.mat\nninaPro/s4/\nninaPro/s4/S4_E1_A1.mat\nninaPro/s4/S4_E2_A1.mat\nninaPro/s4/S4_E3_A1.mat\nninaPro/s1.zip\nninaPro/s9/\nninaPro/s9/S9_E1_A1.mat\nninaPro/s9/S9_E2_A1.mat\nninaPro/s9/S9_E3_A1.mat\nninaPro/s9.zip\nninaPro/s8/\nninaPro/s8/S8_E3_A1.mat\nninaPro/s8/S8_E2_A1.mat\nninaPro/s8/S8_E1_A1.mat\nninaPro/s1/\nninaPro/s1/S1_E2_A1.mat\nninaPro/s1/S1_E3_A1.mat\nninaPro/s1/S1_E1_A1.mat\nninaPro/s2.zip\nninaPro/s3/\nninaPro/s3/S3_E2_A1.mat\nninaPro/s3/S3_E3_A1.mat\nninaPro/s3/S3_E1_A1.mat\nninaPro/s4.zip\nninaPro/s10/\nninaPro/s10/S10_E2_A1.mat\nninaPro/s10/S10_E1_A1.mat\nninaPro/s10/S10_E3_A1.mat\nninaPro/s7.zip\nninaPro/s5.zip\nninaPro/s6.zip\nninaPro/s8.zip\nninaPro/s5/\nninaPro/s5/S5_E3_A1.mat\nninaPro/s5/S5_E1_A1.mat\nninaPro/s5/S5_E2_A1.mat\nninaPro/s7/\nninaPro/s7/S7_E1_A1.mat\nninaPro/s7/S7_E3_A1.mat\nninaPro/s7/S7_E2_A1.mat\n6353315\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\n\n\nclass Lookahead(tf.keras.optimizers.Optimizer):\n\n    def __init__(self,\n                 optimizer,\n                 sync_period: int = 6,\n                 slow_step_size = 0.5,\n                 name: str = \"Lookahead\",\n                 **kwargs):\n        super().__init__(name, **kwargs)\n\n        if isinstance(optimizer, str):\n            optimizer = tf.keras.optimizers.get(optimizer)\n        if not isinstance(optimizer, tf.keras.optimizers.Optimizer):\n            raise TypeError(\n                \"optimizer is not an object of tf.keras.optimizers.Optimizer\")\n\n        self._optimizer = optimizer\n        self._initialized = False\n\n    def _create_slots(self, var_list):\n        self._optimizer._create_slots(var_list=var_list)\n        for var in var_list:\n            self.add_slot(var, 'slow')\n\n    def _create_hypers(self):\n        self._optimizer._create_hypers()\n\n    def _prepare(self, var_list):\n        return self._optimizer._prepare(var_list=var_list)\n\n    def apply_gradients(self, grads_and_vars, name=None):\n        self._optimizer._iterations = self.iterations\n        return super().apply_gradients(grads_and_vars, name)\n\n    def _init_op(self, var):\n        slow_var = self.get_slot(var, 'slow')\n        return slow_var.assign(\n            tf.where(\n                tf.equal(self.iterations,\n                         tf.constant(0, dtype=self.iterations.dtype)),\n                var,\n                slow_var,\n            ),\n            use_locking=self._use_locking)\n\n    def _look_ahead_op(self, var):\n        var_dtype = var.dtype.base_dtype\n        slow_var = self.get_slot(var, 'slow')\n        local_step = tf.cast(self.iterations + 1, tf.dtypes.int64)\n        sync_period = self._get_hyper('sync_period', tf.dtypes.int64)\n        slow_step_size = self._get_hyper('slow_step_size', var_dtype)\n        step_back = slow_var + slow_step_size * (var - slow_var)\n        sync_cond = tf.equal(\n            tf.math.floordiv(local_step, sync_period) * sync_period,\n            local_step)\n        with tf.control_dependencies([step_back]):\n            slow_update = slow_var.assign(\n                tf.where(\n                    sync_cond,\n                    step_back,\n                    slow_var,\n                ),\n                use_locking=self._use_locking)\n            var_update = var.assign(\n                tf.where(\n                    sync_cond,\n                    step_back,\n                    var,\n                ),\n                use_locking=self._use_locking)\n        return tf.group(slow_update, var_update)\n\n    @property\n    def weights(self):\n        return self._weights + self._optimizer.weights\n\n    def _resource_apply_dense(self, grad, var):\n        init_op = self._init_op(var)\n        with tf.control_dependencies([init_op]):\n            train_op = self._optimizer._resource_apply_dense(grad, var)\n            with tf.control_dependencies([train_op]):\n                look_ahead_op = self._look_ahead_op(var)\n        return tf.group(init_op, train_op, look_ahead_op)\n\n    def _resource_apply_sparse(self, grad, var, indices):\n        init_op = self._init_op(var)\n        with tf.control_dependencies([init_op]):\n            train_op = self._optimizer._resource_apply_sparse(\n                grad, var, indices)\n            with tf.control_dependencies([train_op]):\n                look_ahead_op = self._look_ahead_op(var)\n        return tf.group(init_op, train_op, look_ahead_op)\n\n    def get_config(self):\n        config = {\n            'optimizer': tf.keras.optimizers.serialize(self._optimizer),\n            'sync_period': self._serialize_hyperparameter('sync_period'),\n            'slow_step_size': self._serialize_hyperparameter('slow_step_size'),\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    @property\n    def learning_rate(self):\n        return self._optimizer._get_hyper('learning_rate')\n\n    @learning_rate.setter\n    def learning_rate(self, learning_rate):\n        self._optimizer._set_hyper('learning_rate', learning_rate)\n\n    @property\n    def lr(self):\n        return self.learning_rate\n\n    @lr.setter\n    def lr(self, lr):\n        self.learning_rate = lr\n\n    @classmethod\n    def from_config(cls, config, custom_objects=None):\n        optimizer = tf.keras.optimizers.deserialize(\n            config.pop('optimizer'),\n            custom_objects=custom_objects,\n        )\n        return cls(optimizer, **config)","metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1706293851641,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"3vQbtNdYpEAG","execution":{"iopub.status.busy":"2024-02-04T12:46:59.594752Z","iopub.execute_input":"2024-02-04T12:46:59.595145Z","iopub.status.idle":"2024-02-04T12:47:13.778369Z","shell.execute_reply.started":"2024-02-04T12:46:59.595109Z","shell.execute_reply":"2024-02-04T12:47:13.777188Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def Ranger(sync_period=6,\n           slow_step_size=0.5,\n           learning_rate=0.001,\n           beta_1=0.9,\n           beta_2=0.999,\n           epsilon=1e-7,\n           weight_decay=0.,\n           amsgrad=False,\n           sma_threshold=5.0,\n           total_steps=0,\n           warmup_proportion=0.1,\n           min_lr=0.,\n           name=\"Ranger\"):\n    inner = tf.keras.optimizers.Adam(\n            learning_rate=0.001,\n            beta_1=beta_1,\n            beta_2=beta_2,\n            epsilon=epsilon,\n            amsgrad=amsgrad,\n            name=name + \"_Adam\",\n        )\n    optim = Lookahead(inner, sync_period, slow_step_size, name)\n    return inner\n","metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1706293851641,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"81-kkfEbo-km","execution":{"iopub.status.busy":"2024-02-04T12:47:13.780203Z","iopub.execute_input":"2024-02-04T12:47:13.780877Z","iopub.status.idle":"2024-02-04T12:47:13.788553Z","shell.execute_reply.started":"2024-02-04T12:47:13.780843Z","shell.execute_reply":"2024-02-04T12:47:13.787381Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.utils import to_categorical, Sequence\n\n\n\nclass generator(Sequence):\n    def __init__(\n        self,\n        d: dataset,\n        repetitions,\n        shuffle = True,\n        batch_size = 128,\n        imu = False,\n        augment = True,\n        ma = True,\n        has_zero = True\n    ) -> None:\n        self.shuffle = shuffle\n        self.augment = augment\n        self.ma = ma\n        self.ma_len = d.ma\n        self.repetitions = repetitions\n        self.batch_size = batch_size\n        self.imu = imu\n        self.has_zero = has_zero\n        idxs = np.where(np.isin(d.repetition, np.array(repetitions)))\n        self.X = d.emg[idxs].copy()\n        if imu:\n            self.X = np.concatenate([self.X.copy(), d.imu[idxs].copy()], axis=-1)\n        self.y = to_categorical(d.labels[idxs].copy())\n        self.on_epoch_end()\n\n    def __len__(self):\n        \"number of batches per epoch\"\n        return int(np.floor(self.X.shape[0] / self.batch_size))\n\n    def on_epoch_end(self):\n        self.indexes = np.arange(self.X.shape[0])\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n    def __getitem__(self, index):\n        'generate a single batch'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        out = self.X[indexes,:,:].copy()\n        if self.augment:\n            for i in range(out.shape[0]):\n                if self.has_zero:\n                    if self.y[i,0] == 1:\n                        out[i,:,:] = out[i,:,:]\n                    else:\n                        out[i,:,:]=add_noise_random(out[i,:,:])\n                else:\n                    out[i,:,:] = add_noise_random(out[i,:,:])\n        if self.ma:\n            out = np.moveaxis(ma_batch(out, self.ma_len), -1, 0)\n        return out,  self.y[indexes,:]\n","metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1706293851642,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"CjEh_mKqpfxG","execution":{"iopub.status.busy":"2024-02-04T12:47:13.790167Z","iopub.execute_input":"2024-02-04T12:47:13.791276Z","iopub.status.idle":"2024-02-04T12:47:13.833055Z","shell.execute_reply.started":"2024-02-04T12:47:13.791229Z","shell.execute_reply":"2024-02-04T12:47:13.832018Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from typing import Union, Callable, List\n\nimport numpy as np\nimport tensorflow as tf\n\n\nNumber = Union[\n    float,\n    int,\n    np.float16,\n    np.float32,\n    np.float64,\n    np.int8,\n    np.int16,\n    np.int32,\n    np.int64,\n    np.uint8,\n    np.uint16,\n    np.uint32,\n    np.uint64,\n]\n\nInitializer = Union[None, dict, str, Callable]\nRegularizer = Union[None, dict, str, Callable]\nConstraint = Union[None, dict, str, Callable]\nActivation = Union[None, str, Callable]\n\nTensorLike = Union[List[Union[Number, list]], tuple, Number, np.ndarray, tf.Tensor]\nFloatTensorLike = Union[tf.Tensor, float, np.float16, np.float32, np.float64]\nAcceptableDTypes = Union[tf.DType, np.dtype, type, int, str, None]\n","metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1706293851642,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"R6g3O5_tLozX","execution":{"iopub.status.busy":"2024-02-04T12:47:13.834462Z","iopub.execute_input":"2024-02-04T12:47:13.835076Z","iopub.status.idle":"2024-02-04T12:47:13.852719Z","shell.execute_reply.started":"2024-02-04T12:47:13.835030Z","shell.execute_reply":"2024-02-04T12:47:13.851425Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\n\nclass Mish(Layer):\n    def __init__(self, **kwargs):\n        super(Mish, self).__init__(**kwargs)\n        self.supports_masking = True\n\n    def call(self, inputs):\n        return inputs * K.tanh(K.softplus(inputs))\n\n    def get_config(self):\n        base_config = super(Mish, self).get_config()\n        return dict(list(base_config.items()))\n\n    def compute_output_shape(self, input_shape):\n            return input_shape\n\n\n@tf.function\ndef sparsemax(logits: TensorLike, axis: int = -1) -> tf.Tensor:\n    logits = tf.convert_to_tensor(logits, name=\"logits\")\n    shape = logits.get_shape()\n    rank = shape.rank\n    is_last_axis = (axis == -1) or (axis == rank - 1)\n\n    if is_last_axis:\n        output = _compute_2d_sparsemax(logits)\n        output.set_shape(shape)\n        return output\n    rank_op = tf.rank(logits)\n    axis_norm = axis % rank\n    logits = _swap_axis(logits, axis_norm, tf.math.subtract(rank_op, 1))\n    output = _compute_2d_sparsemax(logits)\n    output = _swap_axis(output, axis_norm, tf.math.subtract(rank_op, 1))\n    output.set_shape(shape)\n    return output\n\n\ndef _swap_axis(logits, dim_index, last_index, **kwargs):\n    return tf.transpose(\n        logits,\n        tf.concat(\n            [\n                tf.range(dim_index),\n                [last_index],\n                tf.range(dim_index + 1, last_index),\n                [dim_index],\n            ],\n            0,\n        ),\n        **kwargs,\n    )\n\n@tf.function\ndef _compute_2d_sparsemax(logits):\n    shape_op = tf.shape(logits)\n    obs = tf.math.reduce_prod(shape_op[:-1])\n    dims = shape_op[-1]\n    z = tf.reshape(logits, [obs, dims])\n    z_sorted, _ = tf.nn.top_k(z, k=dims)\n    z_cumsum = tf.math.cumsum(z_sorted, axis=-1)\n    k = tf.range(1, tf.cast(dims, logits.dtype) + 1, dtype=logits.dtype)\n    z_check = 1 + k * z_sorted > z_cumsum\n    k_z = tf.math.reduce_sum(tf.cast(z_check, tf.int32), axis=-1)\n    k_z_safe = tf.math.maximum(k_z, 1)\n    indices = tf.stack([tf.range(0, obs), tf.reshape(k_z_safe, [-1]) - 1], axis=1)\n    tau_sum = tf.gather_nd(z_cumsum, indices)\n    tau_z = (tau_sum - 1) / tf.cast(k_z, logits.dtype)\n    p = tf.math.maximum(tf.cast(0, logits.dtype), z - tf.expand_dims(tau_z, -1))\n    p_safe = tf.where(\n        tf.expand_dims(\n            tf.math.logical_or(tf.math.equal(k_z, 0), tf.math.is_nan(z_cumsum[:, -1])),\n            axis=-1,\n        ),\n        tf.fill([obs, dims], tf.cast(float(\"nan\"), logits.dtype)),\n        p,\n    )\n    p_safe = tf.reshape(p_safe, shape_op)\n    return p_safe\n\n\ndef sparsemax_loss(\n    logits: TensorLike,\n    sparsemax: TensorLike,\n    labels: TensorLike,\n    name = None,\n) -> tf.Tensor:\n    logits = tf.convert_to_tensor(logits, name=\"logits\")\n    sparsemax = tf.convert_to_tensor(sparsemax, name=\"sparsemax\")\n    labels = tf.convert_to_tensor(labels, name=\"labels\")\n    z = logits\n    sum_s = tf.where(\n        tf.math.logical_or(sparsemax > 0, tf.math.is_nan(sparsemax)),\n        sparsemax * (z - 0.5 * sparsemax),\n        tf.zeros_like(sparsemax),\n    )\n    q_part = labels * (0.5 * labels - z)\n    q_part_safe = tf.where(\n        tf.math.logical_and(tf.math.equal(labels, 0), tf.math.is_inf(z)),\n        tf.zeros_like(z),\n        q_part,\n    )\n\n    return tf.math.reduce_sum(sum_s + q_part_safe, axis=1)\n\n\n@tf.function\ndef sparsemax_loss_from_logits(\n    y_true:TensorLike, logits_pred:TensorLike\n) -> tf.Tensor:\n    y_pred = sparsemax(logits_pred)\n    loss = sparsemax_loss(logits_pred, y_pred, y_true)\n    return loss\n\n\nclass SparsemaxLoss(tf.keras.losses.Loss):\n    def __init__(\n        self,\n        from_logits: bool = True,\n        reduction: str = tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE,\n        name: str = \"sparsemax_loss\",\n    ):\n        if from_logits is not True:\n            raise ValueError(\"from_logits must be True\")\n\n        super().__init__(name=name, reduction=reduction)\n        self.from_logits = from_logits\n\n    def call(self, y_true, y_pred):\n        return sparsemax_loss_from_logits(y_true, y_pred)\n\n    def get_config(self):\n        config = {\n            \"from_logits\": self.from_logits,\n        }\n        base_config = super().get_config()\n        return {**base_config, **config}\n","metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1706293851642,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"gdjW8eERLoxt","execution":{"iopub.status.busy":"2024-02-04T12:47:13.854431Z","iopub.execute_input":"2024-02-04T12:47:13.855122Z","iopub.status.idle":"2024-02-04T12:47:13.891161Z","shell.execute_reply.started":"2024-02-04T12:47:13.855079Z","shell.execute_reply":"2024-02-04T12:47:13.889990Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras import backend as K\n\n\nclass OneCycleLR(Callback):\n    def __init__(self,\n                 max_lr,\n                 end_percentage=0.1,\n                 scale_percentage=None,\n                 maximum_momentum=0.95,\n                 minimum_momentum=0.85,\n                 verbose=True):\n        super(OneCycleLR, self).__init__()\n\n        if end_percentage < 0. or end_percentage > 1.:\n            raise ValueError(\"`end_percentage` must be between 0 and 1\")\n\n        if scale_percentage is not None and (scale_percentage < 0. or scale_percentage > 1.):\n            raise ValueError(\"`scale_percentage` must be between 0 and 1\")\n\n        self.initial_lr = max_lr\n        self.end_percentage = end_percentage\n        self.scale = float(scale_percentage) if scale_percentage is not None else float(end_percentage)\n        self.max_momentum = maximum_momentum\n        self.min_momentum = minimum_momentum\n        self.verbose = verbose\n\n        if self.max_momentum is not None and self.min_momentum is not None:\n            self._update_momentum = True\n        else:\n            self._update_momentum = False\n\n        self.clr_iterations = 0.\n        self.history = {}\n\n        self.epochs = None\n        self.batch_size = None\n        self.samples = None\n        self.steps = None\n        self.num_iterations = None\n        self.mid_cycle_id = None\n\n    def _reset(self):\n        self.clr_iterations = 0.\n        self.history = {}\n\n    def compute_lr(self):\n        if self.clr_iterations > 2 * self.mid_cycle_id:\n            current_percentage = (self.clr_iterations - 2 * self.mid_cycle_id)\n            current_percentage /= float((self.num_iterations - 2 * self.mid_cycle_id))\n            new_lr = self.initial_lr * (1. + (current_percentage *\n                                              (1. - 100.) / 100.)) * self.scale\n\n        elif self.clr_iterations > self.mid_cycle_id:\n            current_percentage = 1. - (\n                self.clr_iterations - self.mid_cycle_id) / self.mid_cycle_id\n            new_lr = self.initial_lr * (1. + current_percentage *\n                                        (self.scale * 100 - 1.)) * self.scale\n\n        else:\n            current_percentage = self.clr_iterations / self.mid_cycle_id\n            new_lr = self.initial_lr * (1. + current_percentage *\n                                        (self.scale * 100 - 1.)) * self.scale\n\n        if self.clr_iterations == self.num_iterations:\n            self.clr_iterations = 0\n\n        return new_lr\n\n    def compute_momentum(self):\n        if self.clr_iterations > 2 * self.mid_cycle_id:\n            new_momentum = self.max_momentum\n\n        elif self.clr_iterations > self.mid_cycle_id:\n            current_percentage = 1. - ((self.clr_iterations - self.mid_cycle_id) / float(\n                                        self.mid_cycle_id))\n            new_momentum = self.max_momentum - current_percentage * (\n                self.max_momentum - self.min_momentum)\n\n        else:\n            current_percentage = self.clr_iterations / float(self.mid_cycle_id)\n            new_momentum = self.max_momentum - current_percentage * (\n                self.max_momentum - self.min_momentum)\n\n        return new_momentum\n\n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        self.epochs = self.params['epochs']\n        self.batch_size = self.params['batch_size']\n        self.samples = self.params['samples']\n        self.steps = self.params['steps']\n\n        if self.steps is not None:\n            self.num_iterations = self.epochs * self.steps\n        else:\n            if (self.samples % self.batch_size) == 0:\n                remainder = 0\n            else:\n                remainder = 1\n            self.num_iterations = (self.epochs + remainder) * self.samples // self.batch_size\n\n        self.mid_cycle_id = int(self.num_iterations * ((1. - self.end_percentage)) / float(2))\n\n        self._reset()\n        K.set_value(self.model.optimizer.lr, self.compute_lr())\n\n        if self._update_momentum:\n            if not hasattr(self.model.optimizer, 'momentum'):\n                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n\n            new_momentum = self.compute_momentum()\n            K.set_value(self.model.optimizer.momentum, new_momentum)\n\n    def on_batch_end(self, epoch, logs=None):\n        logs = logs or {}\n\n        self.clr_iterations += 1\n        new_lr = self.compute_lr()\n\n        self.history.setdefault('lr', []).append(\n            K.get_value(self.model.optimizer.lr))\n        K.set_value(self.model.optimizer.lr, new_lr)\n\n        if self._update_momentum:\n            if not hasattr(self.model.optimizer, 'momentum'):\n                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n\n            new_momentum = self.compute_momentum()\n\n            self.history.setdefault('momentum', []).append(\n                K.get_value(self.model.optimizer.momentum))\n            K.set_value(self.model.optimizer.momentum, new_momentum)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n    def on_epoch_end(self, epoch, logs=None):\n        if self.verbose:\n            if self._update_momentum:\n                print(\" - lr: %0.5f - momentum: %0.2f \" %\n                      (self.history['lr'][-1], self.history['momentum'][-1]))\n\n            else:\n                print(\" - lr: %0.5f \" % (self.history['lr'][-1]))\n\n\nclass LRFinder(Callback):\n    def __init__(self,\n                 num_samples,\n                 batch_size,\n                 minimum_lr=1e-5,\n                 maximum_lr=10.,\n                 lr_scale='exp',\n                 validation_data=None,\n                 validation_sample_rate=5,\n                 stopping_criterion_factor=4.,\n                 loss_smoothing_beta=0.98,\n                 save_dir=None,\n                 verbose=True):\n        super(LRFinder, self).__init__()\n\n        if lr_scale not in ['exp', 'linear']:\n            raise ValueError(\"`lr_scale` must be one of ['exp', 'linear']\")\n\n        if validation_data is not None:\n            self.validation_data = validation_data\n            self.use_validation_set = True\n\n            if validation_sample_rate > 0 or validation_sample_rate < 0:\n                self.validation_sample_rate = validation_sample_rate\n            else:\n                raise ValueError(\"`validation_sample_rate` must be a positive or negative integer other than o\")\n        else:\n            self.use_validation_set = False\n            self.validation_sample_rate = 0\n\n        self.num_samples = num_samples\n        self.batch_size = batch_size\n        self.initial_lr = minimum_lr\n        self.final_lr = maximum_lr\n        self.lr_scale = lr_scale\n        self.stopping_criterion_factor = stopping_criterion_factor\n        self.loss_smoothing_beta = loss_smoothing_beta\n        self.save_dir = save_dir\n        self.verbose = verbose\n\n        self.num_batches_ = num_samples // batch_size\n        self.current_lr_ = minimum_lr\n\n        if lr_scale == 'exp':\n            self.lr_multiplier_ = (maximum_lr / float(minimum_lr)) ** (\n                1. / float(self.num_batches_))\n        else:\n            extra_batch = int((num_samples % batch_size) != 0)\n            self.lr_multiplier_ = np.linspace(\n                minimum_lr, maximum_lr, num=self.num_batches_ + extra_batch)\n        if self.validation_sample_rate < 0:\n            self.validation_sample_rate = self.validation_data[0].shape[0] // batch_size\n\n        self.current_batch_ = 0\n        self.current_epoch_ = 0\n        self.best_loss_ = 1e6\n        self.running_loss_ = 0.\n\n        self.history = {}\n\n    def on_train_begin(self, logs=None):\n\n        self.current_epoch_ = 1\n        K.set_value(self.model.optimizer.lr, self.initial_lr)\n\n        warnings.simplefilter(\"ignore\")\n\n    def on_epoch_begin(self, epoch, logs=None):\n        self.current_batch_ = 0\n\n        if self.current_epoch_ > 1:\n            warnings.warn(\n                \"\\n\\nLearning rate finder should be used only with a single epoch. \"\n                \"Hereafter, the callback will not measure the losses.\\n\\n\")\n\n    def on_batch_begin(self, batch, logs=None):\n        self.current_batch_ += 1\n\n    def on_batch_end(self, batch, logs=None):\n        if self.current_epoch_ > 1:\n            return\n\n        if self.use_validation_set:\n            X, Y = self.validation_data[0], self.validation_data[1]\n\n            num_samples = self.batch_size * self.validation_sample_rate\n\n            if num_samples > X.shape[0]:\n                num_samples = X.shape[0]\n\n            idx = np.random.choice(X.shape[0], num_samples, replace=False)\n            x = X[idx]\n            y = Y[idx]\n\n            values = self.model.evaluate(x, y, batch_size=self.batch_size, verbose=False)\n            loss = values[0]\n        else:\n            loss = logs['loss']\n\n\n        running_loss = self.loss_smoothing_beta * loss + (\n            1. - self.loss_smoothing_beta) * loss\n        running_loss = running_loss / (\n            1. - self.loss_smoothing_beta**self.current_batch_)\n\n\n        if self.current_batch_ > 1 and self.stopping_criterion_factor is not None and (\n                running_loss >\n                self.stopping_criterion_factor * self.best_loss_):\n\n            if self.verbose:\n                print(\" - LRFinder: Skipping iteration since loss is %d times as large as best loss (%0.4f)\"\n                      % (self.stopping_criterion_factor, self.best_loss_))\n            return\n\n        if running_loss < self.best_loss_ or self.current_batch_ == 1:\n            self.best_loss_ = running_loss\n\n        current_lr = K.get_value(self.model.optimizer.lr)\n\n        self.history.setdefault('running_loss_', []).append(running_loss)\n        if self.lr_scale == 'exp':\n            self.history.setdefault('log_lrs', []).append(np.log10(current_lr))\n        else:\n            self.history.setdefault('log_lrs', []).append(current_lr)\n\n\n        if self.lr_scale == 'exp':\n            current_lr *= self.lr_multiplier_\n        else:\n            current_lr = self.lr_multiplier_[self.current_batch_ - 1]\n\n        K.set_value(self.model.optimizer.lr, current_lr)\n\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n        if self.verbose:\n            if self.use_validation_set:\n                print(\" - LRFinder: val_loss: %1.4f - lr = %1.8f \" %\n                      (values[0], current_lr))\n            else:\n                print(\" - LRFinder: lr = %1.8f \" % current_lr)\n\n    def on_epoch_end(self, epoch, logs=None):\n        if self.save_dir is not None and self.current_epoch_ <= 1:\n            if not os.path.exists(self.save_dir):\n                os.makedirs(self.save_dir)\n\n            losses_path = os.path.join(self.save_dir, 'losses.npy')\n            lrs_path = os.path.join(self.save_dir, 'lrs.npy')\n\n            np.save(losses_path, self.losses)\n            np.save(lrs_path, self.lrs)\n\n            if self.verbose:\n                print(\"\\tLR Finder : Saved the losses and learning rate values in path : {%s}\"\n                      % (self.save_dir))\n\n        self.current_epoch_ += 1\n\n        warnings.simplefilter(\"default\")\n\n    def plot_schedule(self, clip_beginning=None, clip_endding=None):\n\n        try:\n            import matplotlib.pyplot as plt\n            plt.style.use('seaborn-white')\n        except ImportError:\n            print(\n                \"Matplotlib not found. Please use `pip install matplotlib` first.\"\n            )\n            return\n\n        if clip_beginning is not None and clip_beginning < 0:\n            clip_beginning = -clip_beginning\n\n        if clip_endding is not None and clip_endding > 0:\n            clip_endding = -clip_endding\n\n        losses = self.losses\n        lrs = self.lrs\n        if clip_beginning:\n            losses = losses[clip_beginning:]\n            lrs = lrs[clip_beginning:]\n\n        if clip_endding:\n            losses = losses[:clip_endding]\n            lrs = lrs[:clip_endding]\n\n        plt.plot(lrs, losses)\n        plt.title('Learning rate vs Loss')\n        plt.xlabel('learning rate')\n        plt.ylabel('loss')\n        plt.show()\n\n    @classmethod\n    def restore_schedule_from_dir(cls,\n                                  directory,\n                                  clip_beginning=None,\n                                  clip_endding=None):\n        if clip_beginning is not None and clip_beginning < 0:\n            clip_beginning = -clip_beginning\n\n        if clip_endding is not None and clip_endding > 0:\n            clip_endding = -clip_endding\n\n        losses_path = os.path.join(directory, 'losses.npy')\n        lrs_path = os.path.join(directory, 'lrs.npy')\n\n        if not os.path.exists(losses_path) or not os.path.exists(lrs_path):\n            print(\"%s and %s could not be found at directory : {%s}\" %\n                  (losses_path, lrs_path, directory))\n\n            losses = None\n            lrs = None\n\n        else:\n            losses = np.load(losses_path)\n            lrs = np.load(lrs_path)\n\n            if clip_beginning:\n                losses = losses[clip_beginning:]\n                lrs = lrs[clip_beginning:]\n\n            if clip_endding:\n                losses = losses[:clip_endding]\n                lrs = lrs[:clip_endding]\n\n        return losses, lrs\n\n    @classmethod\n    def plot_schedule_from_file(cls,\n                                directory,\n                                clip_beginning=None,\n                                clip_endding=None):\n\n        try:\n            import matplotlib.pyplot as plt\n            plt.style.use('seaborn-white')\n        except ImportError:\n            print(\"Matplotlib not found. Please use `pip install matplotlib` first.\")\n            return\n\n        losses, lrs = cls.restore_schedule_from_dir(\n            directory,\n            clip_beginning=clip_beginning,\n            clip_endding=clip_endding)\n\n        if losses is None or lrs is None:\n            return\n        else:\n            plt.plot(lrs, losses)\n            plt.title('Learning rate vs Loss')\n            plt.xlabel('learning rate')\n            plt.ylabel('loss')\n            plt.show()\n\n    @property\n    def lrs(self):\n        return np.array(self.history['log_lrs'])\n\n    @property\n    def losses(self):\n        return np.array(self.history['running_loss_'])","metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1706293851642,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"_WsUG8UPLovS","execution":{"iopub.status.busy":"2024-02-04T12:47:13.893368Z","iopub.execute_input":"2024-02-04T12:47:13.894292Z","iopub.status.idle":"2024-02-04T12:47:13.967232Z","shell.execute_reply.started":"2024-02-04T12:47:13.894256Z","shell.execute_reply":"2024-02-04T12:47:13.965990Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n\nx = np.arange(505)\nmi = 1e-5\nma = 1e-3\ngamma = 0.9\nrep = 500\nt = 500\nout = np.zeros(505)\nfor i in range(x.shape[0]):\n    if i>4:\n        out[i] = mi + (ma-mi)*(1+math.cos(math.pi*x[i%(rep+5)]/t))/2\n    else:\n        out[i] = ma\n    if out[i] == mi:\n        ma *= gamma\nplt.plot(out)\nplt.show()\n\n\n","metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1706293851642,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"bHeQFLkqLost","outputId":"c574e2d8-5dff-47af-8497-e03cb30e3787","execution":{"iopub.status.busy":"2024-02-04T15:50:07.730156Z","iopub.execute_input":"2024-02-04T15:50:07.730663Z","iopub.status.idle":"2024-02-04T15:50:08.060344Z","shell.execute_reply.started":"2024-02-04T15:50:07.730597Z","shell.execute_reply":"2024-02-04T15:50:08.059120Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"import math\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras import backend as K\n\n\nclass CosineAnnealingScheduler(Callback):\n    def __init__(self, T_max, eta_max, eta_min=0, verbose=0, epoch_start=80, restart_epochs=None, gamma=1, expansion=1, flat_end = False):\n        super(CosineAnnealingScheduler, self).__init__()\n        self.epoch_start=epoch_start\n        self.expansion=expansion\n        self.T_max = T_max\n        self.eta_max = eta_max\n        self.eta_min = eta_min\n        self.verbose = verbose\n        self.restart_epochs = restart_epochs\n        self.gamma = gamma\n        self.flat_end = flat_end\n\n    def on_epoch_begin(self, epoch, logs=None):\n        if not hasattr(self.model.optimizer, 'learning_rate'):\n            raise ValueError('Optimizer must have a \"learning_rate\" attribute.')\n        if epoch > self.epoch_start - 1:\n            if self.restart_epochs is None:\n                learning_rate = self.eta_min + (self.eta_max*self.gamma - self.eta_min) * (1 + math.cos(math.pi * (epoch - self.epoch_start) / self.T_max)) / 2\n                K.set_value(self.model.optimizer.learning_rate, learning_rate)\n            else:\n                learning_rate = self.eta_min + (self.eta_max*self.gamma - self.eta_min) * (1 + math.cos(math.pi * ((epoch  % (self.restart_epochs+self.epoch_start)) - self.epoch_start) / self.T_max)) / 2\n                K.set_value(self.model.optimizer.learning_rate, learning_rate)\n            if learning_rate<=self.eta_min:\n                self.eta_max *= self.gamma\n                self.T_max *=self.expansion\n        if self.flat_end and epoch >= ((self.epoch_start -1 ) + T_max):\n            learning_rate = self.eta_min\n\n        else:\n            learning_rate=self.model.optimizer.learning_rate\n        if self.verbose > 0:\n            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n                  'rate to %s.' % (epoch + 1, learning_rate))\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs['learning_rate'] = K.get_value(self.model.optimizer.learning_rate)","metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1706293851643,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"nRAC8cC2Loqz","execution":{"iopub.status.busy":"2024-02-04T12:47:13.968703Z","iopub.execute_input":"2024-02-04T12:47:13.969034Z","iopub.status.idle":"2024-02-04T12:47:13.985572Z","shell.execute_reply.started":"2024-02-04T12:47:13.969007Z","shell.execute_reply":"2024-02-04T12:47:13.984545Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import *\n\nclass CyclicLR(Callback):\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n\n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n\n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())\n\n    def on_batch_end(self, epoch, logs=None):\n\n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n        K.set_value(self.model.optimizer.lr, self.clr())","metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1706293851643,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"Ei39enMSLood","execution":{"iopub.status.busy":"2024-02-04T12:47:13.987079Z","iopub.execute_input":"2024-02-04T12:47:13.987571Z","iopub.status.idle":"2024-02-04T12:47:14.012654Z","shell.execute_reply.started":"2024-02-04T12:47:13.987529Z","shell.execute_reply":"2024-02-04T12:47:14.011292Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class LRFinder(Callback):\n    def __init__(self,\n                 num_samples,\n                 batch_size,\n                 minimum_lr=1e-5,\n                 maximum_lr=10.,\n                 lr_scale='exp',\n                 validation_data=None,\n                 validation_sample_rate=5,\n                 stopping_criterion_factor=4.,\n                 loss_smoothing_beta=0.98,\n                 save_dir=None,\n                 verbose=True):\n\n        super(LRFinder, self).__init__()\n\n        if lr_scale not in ['exp', 'linear']:\n            raise ValueError(\"`lr_scale` must be one of ['exp', 'linear']\")\n\n        if validation_data is not None:\n            self.validation_data = validation_data\n            self.use_validation_set = True\n\n            if validation_sample_rate > 0 or validation_sample_rate < 0:\n                self.validation_sample_rate = validation_sample_rate\n            else:\n                raise ValueError(\"`validation_sample_rate` must be a positive or negative integer other than o\")\n        else:\n            self.use_validation_set = False\n            self.validation_sample_rate = 0\n\n        self.num_samples = num_samples\n        self.batch_size = batch_size\n        self.initial_lr = minimum_lr\n        self.final_lr = maximum_lr\n        self.lr_scale = lr_scale\n        self.stopping_criterion_factor = stopping_criterion_factor\n        self.loss_smoothing_beta = loss_smoothing_beta\n        self.save_dir = save_dir\n        self.verbose = verbose\n\n        self.num_batches_ = num_samples // batch_size\n        self.current_lr_ = minimum_lr\n\n        if lr_scale == 'exp':\n            self.lr_multiplier_ = (maximum_lr / float(minimum_lr)) ** (\n                1. / float(self.num_batches_))\n        else:\n            extra_batch = int((num_samples % batch_size) != 0)\n            self.lr_multiplier_ = np.linspace(\n                minimum_lr, maximum_lr, num=self.num_batches_ + extra_batch)\n\n        if self.validation_sample_rate < 0:\n            self.validation_sample_rate = self.validation_data[0].shape[0] // batch_size\n\n        self.current_batch_ = 0\n        self.current_epoch_ = 0\n        self.best_loss_ = 1e6\n        self.running_loss_ = 0.\n\n        self.history = {}\n\n    def on_train_begin(self, logs=None):\n\n        self.current_epoch_ = 1\n        K.set_value(self.model.optimizer.lr, self.initial_lr)\n\n        warnings.simplefilter(\"ignore\")\n\n    def on_epoch_begin(self, epoch, logs=None):\n        self.current_batch_ = 0\n\n        if self.current_epoch_ > 1:\n            warnings.warn(\n                \"\\n\\nLearning rate finder should be used only with a single epoch. \"\n                \"Hereafter, the callback will not measure the losses.\\n\\n\")\n\n    def on_batch_begin(self, batch, logs=None):\n        self.current_batch_ += 1\n\n    def on_batch_end(self, batch, logs=None):\n        if self.current_epoch_ > 1:\n            return\n\n        if self.use_validation_set:\n            X, Y = self.validation_data[0], self.validation_data[1]\n\n\n            num_samples = self.batch_size * self.validation_sample_rate\n\n            if num_samples > X.shape[0]:\n                num_samples = X.shape[0]\n\n            idx = np.random.choice(X.shape[0], num_samples, replace=False)\n            x = X[idx]\n            y = Y[idx]\n\n            values = self.model.evaluate(x, y, batch_size=self.batch_size, verbose=False)\n            loss = values[0]\n        else:\n            loss = logs['loss']\n\n\n        running_loss = self.loss_smoothing_beta * loss + (\n            1. - self.loss_smoothing_beta) * loss\n        running_loss = running_loss / (\n            1. - self.loss_smoothing_beta**self.current_batch_)\n\n\n        if self.current_batch_ > 1 and self.stopping_criterion_factor is not None and (\n                running_loss >\n                self.stopping_criterion_factor * self.best_loss_):\n\n            if self.verbose:\n                print(\" - LRFinder: Skipping iteration since loss is %d times as large as best loss (%0.4f)\"\n                      % (self.stopping_criterion_factor, self.best_loss_))\n            return\n\n        if running_loss < self.best_loss_ or self.current_batch_ == 1:\n            self.best_loss_ = running_loss\n\n        current_lr = K.get_value(self.model.optimizer.lr)\n\n        self.history.setdefault('running_loss_', []).append(running_loss)\n        if self.lr_scale == 'exp':\n            self.history.setdefault('log_lrs', []).append(np.log10(current_lr))\n        else:\n            self.history.setdefault('log_lrs', []).append(current_lr)\n\n\n        if self.lr_scale == 'exp':\n            current_lr *= self.lr_multiplier_\n        else:\n            current_lr = self.lr_multiplier_[self.current_batch_ - 1]\n\n        K.set_value(self.model.optimizer.lr, current_lr)\n\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n        if self.verbose:\n            if self.use_validation_set:\n                print(\" - LRFinder: val_loss: %1.4f - lr = %1.8f \" %\n                      (values[0], current_lr))\n            else:\n                print(\" - LRFinder: lr = %1.8f \" % current_lr)\n\n    def on_epoch_end(self, epoch, logs=None):\n        if self.save_dir is not None and self.current_epoch_ <= 1:\n            if not os.path.exists(self.save_dir):\n                os.makedirs(self.save_dir)\n\n            losses_path = os.path.join(self.save_dir, 'losses.npy')\n            lrs_path = os.path.join(self.save_dir, 'lrs.npy')\n\n            np.save(losses_path, self.losses)\n            np.save(lrs_path, self.lrs)\n\n            if self.verbose:\n                print(\"\\tLR Finder : Saved the losses and learning rate values in path : {%s}\"\n                      % (self.save_dir))\n\n        self.current_epoch_ += 1\n\n        warnings.simplefilter(\"default\")\n\n    def plot_schedule(self, clip_beginning=None, clip_endding=None):\n        try:\n            import matplotlib.pyplot as plt\n            plt.style.use('seaborn-white')\n        except ImportError:\n            print(\n                \"Matplotlib not found. Please use `pip install matplotlib` first.\"\n            )\n            return\n\n        if clip_beginning is not None and clip_beginning < 0:\n            clip_beginning = -clip_beginning\n\n        if clip_endding is not None and clip_endding > 0:\n            clip_endding = -clip_endding\n\n        losses = self.losses\n        lrs = self.lrs\n\n        if clip_beginning:\n            losses = losses[clip_beginning:]\n            lrs = lrs[clip_beginning:]\n\n        if clip_endding:\n            losses = losses[:clip_endding]\n            lrs = lrs[:clip_endding]\n\n        plt.plot(lrs, losses)\n        plt.title('Learning rate vs Loss')\n        plt.xlabel('learning rate')\n        plt.ylabel('loss')\n        plt.show()\n\n    @classmethod\n    def restore_schedule_from_dir(cls,\n                                  directory,\n                                  clip_beginning=None,\n                                  clip_endding=None):\n        if clip_beginning is not None and clip_beginning < 0:\n            clip_beginning = -clip_beginning\n\n        if clip_endding is not None and clip_endding > 0:\n            clip_endding = -clip_endding\n\n        losses_path = os.path.join(directory, 'losses.npy')\n        lrs_path = os.path.join(directory, 'lrs.npy')\n\n        if not os.path.exists(losses_path) or not os.path.exists(lrs_path):\n            print(\"%s and %s could not be found at directory : {%s}\" %\n                  (losses_path, lrs_path, directory))\n\n            losses = None\n            lrs = None\n\n        else:\n            losses = np.load(losses_path)\n            lrs = np.load(lrs_path)\n\n            if clip_beginning:\n                losses = losses[clip_beginning:]\n                lrs = lrs[clip_beginning:]\n\n            if clip_endding:\n                losses = losses[:clip_endding]\n                lrs = lrs[:clip_endding]\n\n        return losses, lrs\n\n    @classmethod\n    def plot_schedule_from_file(cls,\n                                directory,\n                                clip_beginning=None,\n                                clip_endding=None):\n        try:\n            import matplotlib.pyplot as plt\n            plt.style.use('seaborn-white')\n        except ImportError:\n            print(\"Matplotlib not found. Please use `pip install matplotlib` first.\")\n            return\n\n        losses, lrs = cls.restore_schedule_from_dir(\n            directory,\n            clip_beginning=clip_beginning,\n            clip_endding=clip_endding)\n\n        if losses is None or lrs is None:\n            return\n        else:\n            plt.plot(lrs, losses)\n            plt.title('Learning rate vs Loss')\n            plt.xlabel('learning rate')\n            plt.ylabel('loss')\n            plt.show()\n\n    @property\n    def lrs(self):\n        return np.array(self.history['log_lrs'])\n\n    @property\n    def losses(self):\n        return np.array(self.history['running_loss_'])","metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1706293851643,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"QaU3Lh6fLomX","execution":{"iopub.status.busy":"2024-02-04T12:47:14.015196Z","iopub.execute_input":"2024-02-04T12:47:14.015683Z","iopub.status.idle":"2024-02-04T12:47:14.062242Z","shell.execute_reply.started":"2024-02-04T12:47:14.015640Z","shell.execute_reply":"2024-02-04T12:47:14.061139Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf\ntry:\n    import tensorflow.keras as keras\n    import tensorflow.keras.backend as K\nexcept:\n    import keras\n    import keras.backend as K\nimport importlib\nimport gc\n\n\nclass History(object):\n    def __init__(self, history=None):\n        if history is not None:\n            self.epoch = history.epoch\n            self.history = history.history\n        else:\n            self.epoch = []\n            self.history = {}\n\n\ndef concatenate_history(hlist, reindex_epoch=False):\n    his = History()\n\n    for h in hlist:\n        his.epoch = his.epoch + h.epoch\n\n        for key, value in h.history.items():\n            his.history.setdefault(key, [])\n            his.history[key] = his.history[key] + value\n\n    if reindex_epoch:\n        his.epoch = list(np.arange(0, len(his.epoch)))\n\n    return his\n\n\ndef plot_from_history(history):\n    assert isinstance(history, (keras.callbacks.History, History)), \"history must be a ``keras.callbacks.History`` or \" \\\n                                                                    \"(this module's) ``History`` object. \"\n\n    epoch = history.epoch\n    val_exist = \"val_loss\" in history.history\n\n    plt.plot(epoch, history.history[\"loss\"], '.-', label=\"train\")\n    if val_exist:\n        plt.plot(epoch, history.history[\"val_loss\"], '.-', label=\"valid\")\n\n    plt.xlabel('epoch')\n    plt.ylabel('losses')\n    plt.legend()\n\n\ndef save_history_to_csv(history, filepath):\n    hist = history.history\n    hist['epoch'] = history.epoch\n    df = pd.DataFrame.from_dict(hist)\n    df.to_csv(filepath, index=False)\n\n\ndef reset_keras(per_process_gpu_memory_fraction=1.0):\n    sess = K.get_session()\n    K.clear_session()\n    sess.close()\n\n    gc.collect()\n\n\n    config = tf.ConfigProto()\n    config.gpu_options.per_process_gpu_memory_fraction = per_process_gpu_memory_fraction\n    config.gpu_options.visible_device_list = \"0\"\n    K.set_session(tf.Session(config=config))\n\n\ndef cuda_release_memory():\n    spec = importlib.util.find_spec(\"numba\")\n    if spec is None:\n        raise Exception(\"numba module cannot be found. Can't function before numba module is installed.\")\n    else:\n        from numba import cuda\n    cuda.select_device(0)\n    cuda.close()\n    return cuda\n\n\ndef moving_window_avg(x, window=5):\n    return pd.DataFrame(x).rolling(window=window, min_periods=1).mean().values.squeeze()\n\n\ndef set_momentum(optimizer, mom_val):\n    keys = dir(optimizer)\n    if \"momentum\" in keys:\n        K.set_value(optimizer.momentum, mom_val)\n    if \"rho\" in keys:\n        K.set_value(optimizer.rho, mom_val)\n    if \"beta_1\" in keys:\n        K.set_value(optimizer.beta_1, mom_val)\n\n\ndef set_lr(optimizer, lr):\n    K.set_value(optimizer.lr, lr)\n","metadata":{"executionInfo":{"elapsed":682,"status":"ok","timestamp":1706293852302,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"P2SEJ_BDLokG","execution":{"iopub.status.busy":"2024-02-04T12:47:14.064121Z","iopub.execute_input":"2024-02-04T12:47:14.064678Z","iopub.status.idle":"2024-02-04T12:47:14.087940Z","shell.execute_reply.started":"2024-02-04T12:47:14.064635Z","shell.execute_reply":"2024-02-04T12:47:14.086776Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"try:\n    import tensorflow.keras as keras\n    import tensorflow.keras.backend as K\nexcept:\n    import keras\n    import keras.backend as K\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nclass LrRangeTest(keras.callbacks.Callback):\n    def __init__(self,\n                 lr_range=(1e-5, 10),\n                 wd_list=[],\n                 steps=100,\n                 batches_per_step=5,\n                 threshold_multiplier=5,\n                 validation_data=None,\n                 validation_batch_size=16,\n                 batches_per_val=10,\n                 verbose=False):\n\n        super(LrRangeTest, self).__init__()\n\n        self.lr_range = lr_range\n\n        self.wd_list = wd_list\n\n        self.steps = steps\n        self.batches_per_step = batches_per_step\n        self.early_stop = False\n        self.threshold_multiplier = threshold_multiplier\n        self.validation_data = validation_data\n        if validation_data is not None:\n            self.use_validation = True\n        else:\n            self.use_validation = False\n        self.validation_batch_size = validation_batch_size\n        self.batches_per_val = batches_per_val\n        self.verbose = verbose\n        self.lr_values = np.power(10.0,\n                                  np.linspace(np.log10(lr_range[0]), np.log10(lr_range[1]), self.steps))\n\n        self.lr = self.lr_values\n        n_wd = len(self.wd_list) if len(self.wd_list) > 0 else 1\n        self.loss = np.zeros(shape=(self.lr_values.size, n_wd)) * np.nan\n        if self.use_validation:\n            self.val_loss = np.zeros_like(self.loss) * np.nan\n\n        self.current_wd = 0\n        self.model_org = []\n\n        self.current_batches_per_step = 0\n        self.current_loss_val = 0\n\n    def _fetch_val_batch(self, batch):\n        if isinstance(self.validation_data, (tuple,)):\n            batch_size = self.validation_batch_size\n            x = self.validation_data[0][batch * batch_size:(batch + 1) * batch_size]\n            y = self.validation_data[1][batch * batch_size:(batch + 1) * batch_size]\n            return x, y\n        if isinstance(self.validation_data, (keras.utils.Sequence,)):\n            return self.validation_data.__getitem__(batch)\n\n    def _reset(self):\n        self.model.optimizer.set_weights(self.model_org.optimizer.get_weights())\n        self.model.set_weights(self.model_org.get_weights())\n        self.current_step = 0\n        self.current_batches_per_step = 0\n        self.current_loss_val = 0\n        self.best_loss = np.inf\n        self.early_stop = False\n\n    def on_train_begin(self, logs={}):\n        self.model.save(\"lr_range_test_original_stage.h5\")\n        self.model_org = keras.models.load_model(\"lr_range_test_original_stage.h5\")\n        if len(self.wd_list) == 0:\n            self.wd_list = [K.get_value(self.model.optimizer.decay)]\n        self.current_wd = 0\n        self._reset()\n\n    def on_train_batch_begin(self, batch, logs):\n        K.set_value(self.model.optimizer.lr, self.lr_values[self.current_step])\n        K.set_value(self.model.optimizer.decay, self.wd_list[self.current_wd])\n\n    def on_train_batch_end(self, batch, logs):\n\n        self.current_loss_val += logs['loss']\n        self.current_batches_per_step += 1\n\n        if self.current_batches_per_step == self.batches_per_step:\n\n            self.loss[self.current_step, self.current_wd] = self.current_loss_val / self.batches_per_step\n\n            if self.use_validation:\n                self.current_loss_val = 0.0\n                if isinstance(self.validation_data, tuple):\n                    batch_size = self.validation_batch_size\n                    N = int(np.ceil(self.validation_data[0].shape[0] / batch_size))\n                if isinstance(self.validation_data, keras.utils.Sequence):\n                    N = len(self.validation_data)\n                n_batch = min(self.batches_per_val, N)\n                for i in range(n_batch):\n                    data_batch = self._fetch_val_batch(i)\n                    batch_size = data_batch[0].shape[0]\n                    result = self.model.evaluate(x=data_batch[0], y=data_batch[1],\n                                                 batch_size=batch_size,\n                                                 verbose=False)\n                    self.current_loss_val += result[0]\n\n                self.val_loss[self.current_step, self.current_wd] = self.current_loss_val / n_batch\n\n\n            if self.verbose:\n                if not self.use_validation:\n                    print(\"wd={:.2e}\".format(self.wd_list[self.current_wd]), \",\",\n                          \"lr={:.2e}\".format(self.lr_values[self.current_step]), \",\",\n                          \"loss={:.2e}\".format(self.loss[self.current_step - 1, self.current_wd]))\n                if self.use_validation:\n                    print(\"wd={:.2e}\".format(self.wd_list[self.current_wd]), \",\",\n                          \"lr={:.2e}\".format(self.lr_values[self.current_step]), \",\",\n                          \"loss={:.2e}\".format(self.loss[self.current_step - 1, self.current_wd]), \",\",\n                          \"val_loss={:.2e}\".format(self.val_loss[self.current_step - 1, self.current_wd]))\n\n            self.current_batches_per_step = 0\n            self.current_loss_val = 0.0\n            self.current_step += 1\n\n\n            if not self.use_validation:\n                latest_loss = self.loss[self.current_step - 1, self.current_wd]\n            else:\n                latest_loss = self.val_loss[self.current_step - 1, self.current_wd]\n\n            self.best_loss = self.best_loss if self.best_loss < latest_loss else latest_loss\n\n\n            if latest_loss > self.best_loss * self.threshold_multiplier:\n                self.early_stop = True\n\n\n        if self.current_step == self.lr_values.size or self.early_stop:\n            self.current_wd += 1\n            self._reset()\n\n\n        if self.current_wd == len(list(self.wd_list)):\n            self.model.set_weights(self.model_org.get_weights())\n            K.set_value(self.model.optimizer.lr,\n                        K.get_value(self.model_org.optimizer.lr))\n            self.model.optimizer.set_weights(self.model_org.optimizer.get_weights())\n            self.model.stop_training = True\n            try:\n                os.remove(\"lr_range_test_original_stage.h5\")\n            except:\n                pass\n\n    def find_n_epoch(self, dataset, batch_size=None):\n\n        n_wd = len(self.wd_list) if len(self.wd_list) > 0 else 1\n        if isinstance(dataset, keras.utils.Sequence):\n            return int(np.ceil(self.steps * self.batches_per_step / len(dataset)) * n_wd)\n        if isinstance(dataset, np.ndarray):\n            if batch_size is None:\n                raise ValueError(\"``batch_size`` must be provided.\")\n            else:\n                return int(np.ceil(self.steps * self.batches_per_step /\n                                   (dataset.shape[0] / batch_size)) * n_wd)\n\n    def plot(self, set='train', x_scale=\"log\", y_scale=\"linear\", ma=True, window=5, **kwargs):\n\n        assert set in [\"train\", \"valid\"], \"``set`` must be either \"\"train\"\" or \"\"test\"\".\"\n        if set is \"valid\" and not self.use_validation:\n            raise ValueError(\"There is not validation data used to plot. Change set to \"\"train\"\".\")\n        assert x_scale in [\"log\", \"linear\"], \"x_scale must be either \"\"log\"\", or \"\"linear\"\".\"\n        assert y_scale in [\"log\", \"linear\"], \"y_scale must be either \"\"log\"\", or \"\"linear\"\".\"\n\n        plt.figure()\n\n        n_wd = len(self.wd_list) if len(self.wd_list) > 0 else 1\n\n        if set is \"valid\":\n            loss = self.val_loss\n            y_str = \"val loss\"\n        if set is \"train\":\n            loss = self.loss\n            y_str = \"train loss\"\n\n        if ma:\n            loss = np.copy(loss)\n            for i in range(n_wd):\n                loss[:, i] = moving_window_avg(loss[:, i], window=window)\n\n\n        legends = []\n        for w in self.wd_list:\n            legends.append(\"wd={:.1e}\".format(w))\n\n        lr = self.lr\n        plt.plot(lr, loss, **kwargs)\n        plt.xlabel(\"lr\")\n        plt.ylabel(y_str)\n        plt.xscale(x_scale)\n        plt.yscale(y_scale)\n        plt.legend(tuple(legends))\n        plt.show()\n","metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1706293852302,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"VKZ0F5kqLohk","outputId":"295a7a43-456a-4eee-bf3b-a0cd961e9999","execution":{"iopub.status.busy":"2024-02-04T12:47:14.089755Z","iopub.execute_input":"2024-02-04T12:47:14.090446Z","iopub.status.idle":"2024-02-04T12:47:14.137486Z","shell.execute_reply.started":"2024-02-04T12:47:14.090413Z","shell.execute_reply":"2024-02-04T12:47:14.136324Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"<>:172: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n<>:181: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n<>:184: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n<>:172: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n<>:181: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n<>:184: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n/tmp/ipykernel_42/4203049219.py:172: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if set is \"valid\" and not self.use_validation:\n/tmp/ipykernel_42/4203049219.py:181: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if set is \"valid\":\n/tmp/ipykernel_42/4203049219.py:184: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if set is \"train\":\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, Layer\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras import initializers, optimizers, regularizers, constraints\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\n\ndef dot_product(x, kernel):\n    return tf.tensordot(x, kernel, axes=1)\n\nclass Attention(Layer):\n    def __init__(self,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True,\n                 return_attention=False,\n                 **kwargs):\n        self.supports_masking = True\n        self.return_attention = return_attention\n        self.init = initializers.get('zeros')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(Attention, self).__init__(**kwargs)\n\n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'supports_masking':self.supports_masking,\n            'return_attention':self.return_attention,\n            'init':self.init,\n            'W_regularizer':self.W_regularizer,\n            'W_constraint':self.W_constraint,\n            'b_regularizer':self.b_regularizer,\n            'b_constraint':self.b_constraint,\n            'bias':self.bias\n        })\n        return config\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight(shape=(int(input_shape[-1]),),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight(shape=(int(input_shape[1]),),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        eij = dot_product(x, self.W)\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        weighted_input = x * K.expand_dims(a)\n\n        result = K.sum(weighted_input, axis=1)\n\n        if self.return_attention:\n            return [result, a]\n        return result\n\n    def compute_output_shape(self, input_shape):\n        if self.return_attention:\n            return [(input_shape[0], input_shape[-1]),\n                    (input_shape[0], input_shape[1])]\n        else:\n            return input_shape[0], input_shape[-1]\n","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1706293852302,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"9x859lp8LofM","execution":{"iopub.status.busy":"2024-02-04T12:47:14.139370Z","iopub.execute_input":"2024-02-04T12:47:14.140166Z","iopub.status.idle":"2024-02-04T12:47:14.162162Z","shell.execute_reply.started":"2024-02-04T12:47:14.140133Z","shell.execute_reply":"2024-02-04T12:47:14.160879Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nclass FocalLoss(tf.keras.losses.Loss):\n    def __init__(self, gamma=2., alpha=4.,\n                 reduction=tf.keras.losses.Reduction.AUTO, name='focal_loss'):\n\n        super(FocalLoss, self).__init__(reduction=reduction,\n                                        name=name)\n        self.gamma = float(gamma)\n        self.alpha = float(alpha)\n\n    def call(self, y_true, y_pred):\n        epsilon = 1.e-9\n        y_true = tf.convert_to_tensor(y_true, tf.float32)\n        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n\n        model_out = tf.add(y_pred, epsilon)\n        ce = tf.multiply(y_true, -tf.math.log(model_out))\n        weight = tf.multiply(y_true, tf.pow(\n            tf.subtract(1., model_out), self.gamma))\n        fl = tf.multiply(self.alpha, tf.multiply(weight, ce))\n        reduced_fl = tf.reduce_max(fl, axis=1)\n        return tf.reduce_mean(reduced_fl)\n\n","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1706293852302,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"Fc45RgN6LoZ7","execution":{"iopub.status.busy":"2024-02-04T12:47:14.178686Z","iopub.execute_input":"2024-02-04T12:47:14.179297Z","iopub.status.idle":"2024-02-04T12:47:14.193635Z","shell.execute_reply.started":"2024-02-04T12:47:14.179255Z","shell.execute_reply":"2024-02-04T12:47:14.192377Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from os import system, listdir\nimport sys\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    Input,\n    Dense,\n    Dropout,\n    Lambda,\n    Permute,\n    Multiply,\n)\nimu = False","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1706293852302,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"8-oc8KnZLoQ-","execution":{"iopub.status.busy":"2024-02-04T12:47:14.195346Z","iopub.execute_input":"2024-02-04T12:47:14.196354Z","iopub.status.idle":"2024-02-04T12:47:14.208723Z","shell.execute_reply.started":"2024-02-04T12:47:14.196312Z","shell.execute_reply":"2024-02-04T12:47:14.207435Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom EMG_data.generator import generator\nreps = np.unique(data.repetition)\nval_reps = reps[3::2]\ntrain_reps = reps[np.where(np.isin(reps, val_reps, invert=True))]\ntest_reps = val_reps[-1].copy()\nval_reps = val_reps[:-1]\n\ntrain = generator(data, list(train_reps), imu = imu)\nprint(train)\nvalidation = generator(data, list(val_reps), augment=False, imu = imu)\ntest = generator(data, [test_reps][0], augment=False, imu = imu)","metadata":{"executionInfo":{"elapsed":712,"status":"error","timestamp":1706293888475,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"Tt1sr_TpnW6a","execution":{"iopub.status.busy":"2024-02-04T12:47:14.230842Z","iopub.execute_input":"2024-02-04T12:47:14.232012Z","iopub.status.idle":"2024-02-04T12:47:16.712413Z","shell.execute_reply.started":"2024-02-04T12:47:14.231967Z","shell.execute_reply":"2024-02-04T12:47:16.711084Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"<EMG_data.generator.generator object at 0x7d0beed6bdc0>\n","output_type":"stream"}]},{"cell_type":"code","source":"timesteps = train[0][0].shape[1]\nn_class = 53\nn_features = train[0][0].shape[-1]\n\nmodel_pars = {\n    \"timesteps\": timesteps,\n    \"n_class\": n_class,\n    \"n_features\": n_features,\n    \"classifier_architecture\": [500, 500, 2000],\n    \"dropout\": [0.36, 0.36, 0.36],\n}\n\n\n\ndef attention_simple(inputs, timesteps):\n    input_dim = int(inputs.shape[-1])\n    a = Permute((2, 1), name='transpose')(inputs)\n    a = Dense(timesteps, activation='softmax',  name='attention_probs')(a)\n    a_probs = Permute((2, 1), name='attention_vec')(a)\n    output_attention_mul = Multiply(name='focused_attention')([inputs, a_probs])\n    output_flat = Lambda(lambda x: K.sum(x, axis=1), name='temporal_average')(output_attention_mul)\n    return output_flat, a_probs\n\n\ndef dense_model(timesteps, n_class, n_features, classifier_architecture, dropout):\n    inputs = Input((timesteps, n_features))\n    x = Dense(128, activation=Mish())(inputs)\n    x = LayerNormalization()(x)\n    x, a = attention_simple(x, timesteps)\n    for d, dr in zip(classifier_architecture, dropout):\n        x = Dropout(dr)(x)\n        x = Dense(d, activation=Mish())(x)\n        x = LayerNormalization()(x)\n    outputs = Dense(n_class, activation=\"softmax\")(x)\n    model = Model(inputs, outputs)\n    return model\n\n\n\nmodel = dense_model(**model_pars)\n\ncosine = CosineAnnealingScheduler(\n    T_max=50, eta_max=1e-3, eta_min=1e-5, verbose=1, epoch_start=5\n)\nloss = FocalLoss(gamma=3., alpha=6.)\nmodel.compile(Ranger(learning_rate=1e-3), loss=loss, metrics=[\"accuracy\"])\n\nprint(model.summary())\n\nmodel.fit(\n    train,\n    epochs=20,\n    validation_data=validation,\n    callbacks=[\n        ModelCheckpoint(\n            \"main.h5\",\n            monitor=\"val_loss\",\n            keep_best_only=True,\n            save_weights_only=False,\n        ),\n        cosine,\n    ],\n    shuffle = False,\n)\n","metadata":{"executionInfo":{"elapsed":21,"status":"aborted","timestamp":1706293888476,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"P7N6wDGNr7bH","execution":{"iopub.status.busy":"2024-02-04T12:47:16.713793Z","iopub.execute_input":"2024-02-04T12:47:16.714140Z","iopub.status.idle":"2024-02-04T15:50:07.718253Z","shell.execute_reply.started":"2024-02-04T12:47:16.714110Z","shell.execute_reply":"2024-02-04T15:50:07.715534Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_1 (InputLayer)        [(None, 38, 16)]             0         []                            \n                                                                                                  \n dense (Dense)               (None, 38, 128)              2176      ['input_1[0][0]']             \n                                                                                                  \n layer_normalization (Layer  (None, 38, 128)              256       ['dense[0][0]']               \n Normalization)                                                                                   \n                                                                                                  \n transpose (Permute)         (None, 128, 38)              0         ['layer_normalization[0][0]'] \n                                                                                                  \n attention_probs (Dense)     (None, 128, 38)              1482      ['transpose[0][0]']           \n                                                                                                  \n attention_vec (Permute)     (None, 38, 128)              0         ['attention_probs[0][0]']     \n                                                                                                  \n focused_attention (Multipl  (None, 38, 128)              0         ['layer_normalization[0][0]', \n y)                                                                  'attention_vec[0][0]']       \n                                                                                                  \n temporal_average (Lambda)   (None, 128)                  0         ['focused_attention[0][0]']   \n                                                                                                  \n dropout (Dropout)           (None, 128)                  0         ['temporal_average[0][0]']    \n                                                                                                  \n dense_1 (Dense)             (None, 500)                  64500     ['dropout[0][0]']             \n                                                                                                  \n layer_normalization_1 (Lay  (None, 500)                  1000      ['dense_1[0][0]']             \n erNormalization)                                                                                 \n                                                                                                  \n dropout_1 (Dropout)         (None, 500)                  0         ['layer_normalization_1[0][0]'\n                                                                    ]                             \n                                                                                                  \n dense_2 (Dense)             (None, 500)                  250500    ['dropout_1[0][0]']           \n                                                                                                  \n layer_normalization_2 (Lay  (None, 500)                  1000      ['dense_2[0][0]']             \n erNormalization)                                                                                 \n                                                                                                  \n dropout_2 (Dropout)         (None, 500)                  0         ['layer_normalization_2[0][0]'\n                                                                    ]                             \n                                                                                                  \n dense_3 (Dense)             (None, 2000)                 1002000   ['dropout_2[0][0]']           \n                                                                                                  \n layer_normalization_3 (Lay  (None, 2000)                 4000      ['dense_3[0][0]']             \n erNormalization)                                                                                 \n                                                                                                  \n dense_4 (Dense)             (None, 53)                   106053    ['layer_normalization_3[0][0]'\n                                                                    ]                             \n                                                                                                  \n==================================================================================================\nTotal params: 1432967 (5.47 MB)\nTrainable params: 1432967 (5.47 MB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\nNone\n\nEpoch 00001: CosineAnnealingScheduler setting learning rate to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>.\nEpoch 1/20\n6521/6521 [==============================] - 532s 81ms/step - loss: 4.4258 - accuracy: 0.7393 - val_loss: 2.5970 - val_accuracy: 0.8152 - learning_rate: 0.0010\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 00002: CosineAnnealingScheduler setting learning rate to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>.\nEpoch 2/20\n6521/6521 [==============================] - 535s 82ms/step - loss: 2.6861 - accuracy: 0.8016 - val_loss: 2.1563 - val_accuracy: 0.8356 - learning_rate: 0.0010\n\nEpoch 00003: CosineAnnealingScheduler setting learning rate to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>.\nEpoch 3/20\n6521/6521 [==============================] - 527s 81ms/step - loss: 2.1904 - accuracy: 0.8231 - val_loss: 2.0232 - val_accuracy: 0.8431 - learning_rate: 0.0010\n\nEpoch 00004: CosineAnnealingScheduler setting learning rate to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>.\nEpoch 4/20\n6521/6521 [==============================] - 533s 82ms/step - loss: 1.9263 - accuracy: 0.8360 - val_loss: 1.8887 - val_accuracy: 0.8510 - learning_rate: 0.0010\n\nEpoch 00005: CosineAnnealingScheduler setting learning rate to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>.\nEpoch 5/20\n6521/6521 [==============================] - 532s 82ms/step - loss: 1.7364 - accuracy: 0.8455 - val_loss: 1.8871 - val_accuracy: 0.8545 - learning_rate: 0.0010\n\nEpoch 00006: CosineAnnealingScheduler setting learning rate to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>.\nEpoch 6/20\n6521/6521 [==============================] - 527s 81ms/step - loss: 1.6161 - accuracy: 0.8523 - val_loss: 1.8858 - val_accuracy: 0.8545 - learning_rate: 0.0010\n\nEpoch 00007: CosineAnnealingScheduler setting learning rate to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0009990232>.\nEpoch 7/20\n6521/6521 [==============================] - 531s 81ms/step - loss: 1.5079 - accuracy: 0.8582 - val_loss: 1.8422 - val_accuracy: 0.8565 - learning_rate: 9.9902e-04\n\nEpoch 00008: CosineAnnealingScheduler setting learning rate to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0009960968>.\nEpoch 8/20\n6521/6521 [==============================] - 531s 81ms/step - loss: 1.4219 - accuracy: 0.8625 - val_loss: 1.7851 - val_accuracy: 0.8586 - learning_rate: 9.9610e-04\n\nEpoch 00009: CosineAnnealingScheduler setting learning rate to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0009912322>.\nEpoch 9/20\n6521/6521 [==============================] - 531s 81ms/step - loss: 1.3607 - accuracy: 0.8665 - val_loss: 1.7811 - val_accuracy: 0.8608 - learning_rate: 9.9123e-04\n\nEpoch 00010: CosineAnnealingScheduler setting learning rate to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0009844487>.\nEpoch 10/20\n6521/6521 [==============================] - 531s 81ms/step - loss: 1.2993 - accuracy: 0.8699 - val_loss: 1.8054 - val_accuracy: 0.8601 - learning_rate: 9.8445e-04\n\nEpoch 00011: CosineAnnealingScheduler setting learning rate to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.000975773>.\nEpoch 11/20\n6521/6521 [==============================] - 534s 82ms/step - loss: 1.2409 - accuracy: 0.8739 - val_loss: 1.8043 - val_accuracy: 0.8617 - learning_rate: 9.7577e-04\n\nEpoch 00012: CosineAnnealingScheduler setting learning rate to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00096523936>.\nEpoch 12/20\n6521/6521 [==============================] - 536s 82ms/step - loss: 1.2013 - accuracy: 0.8762 - val_loss: 1.7800 - val_accuracy: 0.8630 - learning_rate: 9.6524e-04\n\nEpoch 00013: CosineAnnealingScheduler setting learning rate to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0009528894>.\nEpoch 13/20\n6521/6521 [==============================] - 534s 82ms/step - loss: 1.1585 - accuracy: 0.8792 - val_loss: 1.8011 - val_accuracy: 0.8641 - learning_rate: 9.5289e-04\n\nEpoch 00014: CosineAnnealingScheduler setting learning rate to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0009387718>.\nEpoch 14/20\n6521/6521 [==============================] - 534s 82ms/step - loss: 1.1210 - accuracy: 0.8811 - val_loss: 1.7847 - val_accuracy: 0.8605 - learning_rate: 9.3877e-04\n\nEpoch 00015: CosineAnnealingScheduler setting learning rate to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0009229423>.\nEpoch 15/20\n6521/6521 [==============================] - 535s 82ms/step - loss: 1.0861 - accuracy: 0.8840 - val_loss: 1.8091 - val_accuracy: 0.8634 - learning_rate: 9.2294e-04\n\nEpoch 00016: CosineAnnealingScheduler setting learning rate to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0009054634>.\nEpoch 16/20\n6521/6521 [==============================] - 534s 82ms/step - loss: 1.0534 - accuracy: 0.8859 - val_loss: 1.7681 - val_accuracy: 0.8650 - learning_rate: 9.0546e-04\n\nEpoch 00017: CosineAnnealingScheduler setting learning rate to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0008864041>.\nEpoch 17/20\n6521/6521 [==============================] - 531s 81ms/step - loss: 0.9918 - accuracy: 0.8900 - val_loss: 1.7749 - val_accuracy: 0.8672 - learning_rate: 8.6584e-04\n\nEpoch 00019: CosineAnnealingScheduler setting learning rate to <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00084385084>.\nEpoch 19/20\n6521/6521 [==============================] - 529s 81ms/step - loss: 0.9428 - accuracy: 0.8938 - val_loss: 1.8264 - val_accuracy: 0.8662 - learning_rate: 8.2052e-04\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x7d0bee693310>"},"metadata":{}}]},{"cell_type":"code","source":"train = model.evaluate(validation)\nprint(f\"train data {train}\")\n\ntest = model.evaluate(test)\nprint(f\"Test data {test}\")\n\n","metadata":{"executionInfo":{"elapsed":21,"status":"aborted","timestamp":1706293888476,"user":{"displayName":"Pradeep Kumar Rebbavarapu","userId":"04938182746043963153"},"user_tz":-330},"id":"LAcvkT47xY2L","execution":{"iopub.status.busy":"2024-02-04T15:50:08.062172Z","iopub.execute_input":"2024-02-04T15:50:08.063348Z","iopub.status.idle":"2024-02-04T15:52:52.190569Z","shell.execute_reply.started":"2024-02-04T15:50:08.063303Z","shell.execute_reply":"2024-02-04T15:52:52.189578Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"1572/1572 [==============================] - 81s 52ms/step - loss: 1.8269 - accuracy: 0.8662\ntrain data [1.8269151449203491, 0.8661537766456604]\n1582/1582 [==============================] - 82s 52ms/step - loss: 1.8919 - accuracy: 0.8637\nTest data [1.891939401626587, 0.8637009859085083]\n","output_type":"stream"}]}]}